+++
date = '2025-05-22T10:42:33+08:00'
draft = false
title = '监控优化查询'
+++
<!--more-->
挑战是微信全景监控，功能分为：
- 数据上报：峰值7亿/min
- 数据查询：80w/min，平均响应时间较长：1000ms~2000ms
    - 接口：
        - 维度枚举查询
        - 时间序列查询
<!--more-->

# 预思考

就相当于我们一直使用的智研监控，数据上报/数据查询

立刻去看了下智研监控的耗时，发现拉数据最多800ms，很快

应该是底层做了分区，或者缓存，数据给用户感受还是很快的

# 思想

用户行为分析-> 发现用户99%的请求数据视图，查询1天之前的数据
剩余接口是查询维度值

## 原因

时间跨度大，需要跨多个broker去拿数据再聚合
# 关键点

少查底层数据，对于跨度大的查询，使用缓存+分解子查询

分解子查询

缓存如何做？
总结下：分级缓存，尽量少的读取原数据
- 可以有冗余数据，[天]数据，[小时]数据
- 缓存更新时间：认为阈值时间（缓存未更新时），数据不可信，可信数据可以直接查缓存，不可信的需要查原数据

# 效果
# 衡量标准


# 其他

- Druid（德鲁伊）：数据存储/查询引擎

# 后思考