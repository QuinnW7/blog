~~+++
date = '2025-09-10T17:55:00+08:00'
draft = false
title = 'AI搭子'
+++



<!--more-->

首先，验证动态计划生成器（1.0）；其次，引入社交协作与主动式AI助理功能（2.0）；最终，构建一个由用户贡献内容驱动的模板平台生态（3.0）。
市场切入方面，将优先选择微信小程序作为冷启动平台，充分利用其强大的社交生态和消息触达能力，随后扩展至独立APP及B端API服务。商业模式将采用面向个人消费者（C端）和企业客户（B端）并行的双轨制。



- AI Partner将填补一个关键的市场空白。当前市场在简单的习惯打卡应用和复杂的企业级项目管理工具之间存在一个巨大的断层。前者功能单一，无法应对复杂目标；后者则为企业效率设计，缺乏对个人生活目标和情感动机的关怀。AI Partner将精准切入这一蓝海市场，为个人和小型团体的目标实现提供一个全新的、定制化的解决方案，成为一个介于个人教练、项目经理和社交工具之间的新物种。
![img.png](img.png)

1. 项目名称 & 一句话概括 (Project Title & Elevator Pitch)
   例如：“AI搭子，”
   - 一句话概括：AI Partner并非简单的内容生成器，而是一个能够与用户共同成长的智能伙伴，通过生成可交互、可追踪的个性化行动计划，为用户提供持续的价值。
2. 项目背景 (Situation)
   业务痛点是什么？ 为什么要做这个项目？
   - AI Partner将填补一个关键的市场空白。当前市场在简单的习惯打卡应用和复杂的企业级项目管理工具之间存在一个巨大的断层。前者功能单一，无法应对复杂目标；后者则为企业效率设计，缺乏对个人生活目标和情感动机的关怀。AI Partner将精准切入这一蓝海市场，为个人和小型团体的目标实现提供一个全新的、定制化的解决方案，成为一个介于个人教练、项目经理和社交工具之间的新物种。
   - 超越当前AI工具生成静态、一次性内容的局限，创造出一种动态、持久且可协作的“服务”（Atomic Service）。这一理念旨在解决消费级AI产品普遍存在的两大核心痛点：“上下文缺失”（Context Deficit）和“网络效应壁垒”（Network Effect Barrier）。
3. 我的角色与职责 (Task)
   清晰地说明你是谁，干了什么。
   优秀示范：我作为项目的核心后端开发，主要负责AI计划生成核心链路的设计与实现。具体包括： 设计并实现基于工作流的异步任务编排系统，将复杂的计划生成过程拆解为多个可独立执行、可重试的步骤，包括澄清问题、生成计划概要和搜索微信公众号及生成计划详情的功能。
4. 实施过程与技术方案 (Action)
   这是最核心的技术部分，用点列式说清楚关键的行动。
   技术选型：由于一期的功能流程固定，因此使用工作流搭建整个流程
   架构设计：画出简单的架构图（如果有的话），说明数据流或模块关系。
   关键难点：时间过长，改为并发节点，调用搜索库

5. 项目成果与价值 (Result)
   项目上线后，XX人使用
6. 复盘与反思 (Learning)
   后期会搭建agent kit适配更灵活的场景

# 技术架构
![img_1.png](img_1.png)

# 遇到的问题

## 1、模型最大输出8k,但搜索的时候会将公众号的image_url/url都搜出来，而且随着规划数变大，远超过8k，模型会被截断。怎么处理？

LLM 应用中一个非常经典的 "Retrieve-then-Generate" 模式的优化。将繁重的数据复制任务从 LLM 剥离，让 LLM 只负责最核心的“决策”（选择哪个 item），然后由后端代码来“填充”完整数据。
这样做有几个巨大的好处：
- 减少输出 Token：模型不再需要生成冗长的 content, url 等，显著降低了 API 调用成本和延迟。
- 保证数据准确性：后端代码填充的数据 100% 来自原始来源，避免了 LLM 在复制过程中可能出现的“幻觉”或格式错误。
- 简化 Prompt：Prompt 的指令可以更聚焦于核心任务（规划和选择），降低了模型的理解难度。



. 角色与职责 (Task) -> 具体化
Bad: “后端底层开发负责人，主要负责聊天链路的搭建。”
Good: “我作为项目的核心后端开发（或 Lead a 2-person backend team），主要负责AI计划生成核心链路的设计与实现。具体包括：
设计并实现基于[具体工作流引擎名，如Temporal]的异步任务编排系统，将复杂的计划生成过程拆解为多个可独立执行、可重试的步骤（如：意图理解、知识库检索、LLM内容生成、计划格式化）。
开发LLM网关服务，统一管理对多种大模型（如GPT-4, Claude）的调用，并实现了Prompt模板化管理和成本监控。
主导解决了‘生成耗时过长’和‘Token溢出’两大性能瓶颈，通过并发改造和‘ID-Then-Fill’模式，将平均生成耗时从15秒优化至5秒内。”
2. 实施过程与技术方案 (Action) -> 细节化
   技术选型： 明确说出工作流引擎的名称，并准备好**“为什么是它”**的理由。例如：“我们选用了Temporal，因为它提供了强大的状态持久化和重试机制，非常适合处理这种长达数十秒且需要高可靠性的异步任务。相比之下，简单的消息队列+Worker模式在状态管理和故障恢复上会复杂得多。”
   架构设计图： 在现有图的基础上，可以再准备一张更详细的图，至少要标明：
   关键组件的技术栈 (e.g., Gateway: Go+Gin, Workflow: Java+Temporal, LLM Gateway: Python+FastAPI)
   数据存储 (e.g., PostgreSQL存业务数据, Redis做缓存, Elasticsearch/VectorDB做搜索)
   通信协议 (e.g., RESTful API, gRPC, 消息队列)
   关键难点： 把你的解决方案变成一个生动的技术故事。
   并发优化故事： “最初我们是串行执行：1. 意图分析 -> 2. 向量搜索 -> 3. 调用LLM。发现第2步和第3步经常耗时超过5秒。我的方案是将计划拆解为3个子目标，利用Python的asyncio/Go的goroutine，将3个子目标的‘搜索+LLM调用’过程并发执行。挑战在于需要处理部分失败的情况，我们设计了重试和降级逻辑，即使一个子目标生成失败，也能返回部分可用的计划。最终效果是，P95延迟从15s降低到了5s。”
   Token溢出故事： “我们发现当规划包含10个以上参考文章时，仅URL和标题就占用了几千Token。我的解决方案是改造Prompt，让LLM的角色从‘内容搬运工’转变为‘决策者’。LLM只输出一个包含文章ID的JSON数组，如{'references': [101, 205, 301]}。为此，我设计了一套严格的输出Schema，并利用Function Calling（或JSON Mode）来保证LLM的输出能被后端服务100%正确解析。 后端再根据ID去数据库查询完整信息进行填充。这个改动将Prompt和Completion的Token消耗降低了约70%，大幅节约了成本并避免了截断。”
3. 项目成果与价值 (Result) -> 量化
   这是你简历和面试中最需要加强的部分！没有量化，就没有说服力。
   用户/业务指标 (如果有)：
   “项目上线后，在未做推广的情况下，通过社交分享获得500名种子用户。”
   “用户日均生成计划超过1000次，用户的次日留存率达到20%。”
   技术/性能指标 (必须有)：
   “通过并发优化，核心接口的P95延迟从15秒降低至5秒。”
   “通过‘ID-Then-Fill’方案，单次AI调用成本降低了70%。”
   “系统稳定性达到99.9%，上线一个月内无重大P0/P1事故。”
   如果这是个人项目，没有真实用户：
   “完成了MVP版本的端到端开发和部署，并通过了100个不同场景的测试用例。”
   “压测结果显示，当前架构在xx云服务器配置下，可支持50 QPS的并发请求。”
4. 复盘与反思 (Learning) -> 展现思考深度
   Agent Kit： 准备好对Agent的理解。可以说：“当前的工作流是确定性的编排，而Agent的核心是基于LLM的自主决策和动态规划能力。比如，当一个步骤失败时，Agent可以自己决定是重试、寻找替代方案还是询问用户，这比我们硬编码的重试逻辑要灵活得多。未来引入Agent Kit（如LangChain或AutoGen）可以让我们的AI伙伴更‘智能’，能处理更开放式的任务。”