+++
date = '2025-12-03T10:00:00+08:00'
draft = true
title = '2025.12.03'
+++
<!--more-->

工作：
- 调研python

其他：
- 写cv

我学着学着发现不能这样准备，会疯掉，所以看了下jd，如果跳，就去最火的ai infra，很对口

我现在要梳理下ai infra的全景图，缺啥补啥





--- 
Agent平台算法需求落地
1、响应 DeepSearch 标注接入需求，支持模型调试环境，与 Qlabel 完成系统联调，成功支撑算法同学完成 T1 模型的调试与验证工作
2、探索 DeepResearch-Browser Use 前沿算法需求，完成技术调研及方案可行性验证，制作 Demo 并成功演示，确定落地方案
3、基于 OWL 框架跑通 GAIA 数据集，搭建多模型评测链路，深度适配蒸馏平台及工具，成功跑通 Claude 及 Hunyuan 模型，交付高质量轨迹数据
4、建设 Agent 数据合成框架的工具系统，提供 OWL 工具链支持，完成工具链调研与开发，以标准化文档形式交付，降低后续研发接入与使用成本


# 响应 DeepSearch 标注接入需求，支持模型调试环境
考察的是候选人的 MLOps、数据工程和系统设计能力，重点在于如何打通数据、模型、调试三者之间的闭环。




1. 基础问题 (考察基本概念)
   Q1: 请你描述一下一个典型的“模型-数据”闭环（Model-Data Loop）或“人在回路”（Human-in-the-Loop）系统包含哪些关键组件和流程？

   考察点: 是否理解从模型预测、发现 bad case、数据标注、模型重训到重新上线的整个 MLOps 流程。
2. 设计问题 (考察系统设计和工程能力)
   Q2: 现在需要你来主导设计一个系统，将 DeepSearch 标注平台与我们的模型调试环境进行对接。请问你会如何设计这个系统的整体架构？数据流是怎样的？
   追问1: 如何保证数据的一致性？比如，调试环境中发现的问题数据，如何高效地回流到 DeepSearch，并确保标注人员能准确理解需要修正的内容？
   追问2: 我们的模型迭代非常快，如何设计数据版本管理策略，让每个版本的模型都能对应到它所使用的训练和评测数据集？你会用哪些工具（如 DVC）或方法？
   考察点: 系统设计能力、API 设计思想、数据工程（ETL）、数据版本控制的实践经验。
3. 场景问题 (考察解决实际问题的能力)
   Q3: 在调试环境中，我们发现模型对某一类特定指令的理解总是出错。通过这个系统，你希望从 DeepSearch 获取什么样的新标注数据来解决这个问题？如何设计一个自动化的流程，来“挖掘”更多这类困难样本（Hard Case Mining）并送去标注？
   考察点: 主动学习（Active Learning）的思想，以及将算法思想落地为工程方案的能力。



